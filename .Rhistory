# Nombre: Enrique Onieva Caracuel
# Documento: Aspectos del razonamiento probabilÃ­stico
#---------------------------------------------------------------------------
## PreparaciÃ³n del entorno de trabajo
rm(list = ls())
cat("\014")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
library(ggplot2)
library(mlbench)
library(arules)
data("PimaIndiansDiabetes")
datos = PimaIndiansDiabetes
head(datos)
# CÃ¡lculo de la probabilidad incondicional, hacemos uso del comando table()
tabla = table(datos$diabetes)
tabla
# Tengo que dividir entre el nÃºmero de datos
tabla = tabla/nrow(datos)
tabla
# CÃ¡lculo de la probabilidad condicional para un atributo no numÃ©rico
summary(datos$age)
# Como en estos datos sÃ³lo hay numÃ©ricos, voy a crearme el mÃ­o
datos$joven = datos$age<35
# Y puedo usar la funciÃ³n table(), pero sobre 2 columnas
tabla = table(datos$diabetes,datos$joven)
tabla
# Y ahora tengo que dividir entre el nÃºmero de datos por cada fila
tabla = tabla/rowSums(tabla)
tabla
datos$edadpar = (datos$age %% 2)==0
tabla = table(datos$diabetes,datos$edadpar)
tabla/rowSums(tabla)
datos$edadmultiplode5 = (datos$age %% 5)==0
tabla = table(datos$diabetes,datos$edadmultiplode5)
tabla/rowSums(tabla)
View(datos)
# De la misma manera si tengo mÃ¡s de 2 estados posibles
datos$edad = discretize(datos$age, "fixed", categories = c(20,30,40,50,60,Inf))
rm(list = ls());cat("\014")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# (incluya aquÃ­ cualquier librerÃ­a a utilizar)
install.packages("caret", dependencies = TRUE)
install.packages("manipulateWidget", dependencies = TRUE)
install.packages("R2HTML")
library(dplyr)
library(lattice)
library(ggplot2)
library(caret)
library(e1071)
library(clusterSim)
library(class)
#---------------------------------------------------------------------------
#---------------------------------------------------------------------------
######################## Parte 1 ###########################################
#---------------------------------------------------------------------------
# Paso 1: Utilice la funciÃ³n read.csv para leer el contenido del fichero "movie_metadata.csv"
#     AdemÃ¡s, elimine las siguientes columnas: "director_name", "actor_2_name", "actor_1_name"
#     "movie_title", "plot_keywords", "movie_imdb_link"
# ** Puede automatizar el proceso si intenta eliminar todas las columnas que sean de
#    tipo factor, y que ademÃ¡s, tengan mÃ¡s de 100 niveles diferentes
datos = read.csv("datos/movie_metadata.csv")
datos = dplyr::select(datos, -director_name, -actor_2_name, -actor_1_name, -movie_title, -plot_keywords, -movie_imdb_link)
View(datos)
datos$imdb_score = discretize(datos$imdb_score, "frequency", breaks = c(7.5,9,10))
datos$imdb_score = discretize(datos$imdb_score, "frequency", categories = c(7.5,9,10))
datos$imdb_score = discretize(datos$imdb_score, "fixed", breaks = c(7.5,9,10))
datos = read.csv("datos/movie_metadata.csv")
datos = dplyr::select(datos, -director_name, -actor_2_name, -actor_1_name, -movie_title, -plot_keywords, -movie_imdb_link)
datos$imdb_score = discretize(datos$imdb_score, "fixed", breaks = c(0,7.5,9,10))
d = read.csv("datos/movie_metadata.csv")
View(d)
datos = read.csv("datos/movie_metadata.csv")
datos = dplyr::select(datos, -director_name, -actor_2_name, -actor_1_name, -movie_title, -plot_keywords, -movie_imdb_link)
# Paso 2: Discretice la columna imbd_score para tener rangos en [0, 7.5) [7.5, 9) y [9, 10]
datos$imdb_score = discretize(datos$imdb_score, "fixed", breaks = c(0,7.5,9,10))
# Paso 3: QuÃ©date Ãºnicamente con las filas que tienen todos los datos (no tienen NAs)
datos = na.omit(datos)
# Paso 4: utilice el mÃ©todo "createDataPartition", de la librerÃ­a "caret" para partir los datos
#         en 2 pedazos-> [datostra, con el 80% de los datos] [datostst con el 20% restante]
trainIndex = createDataPartition(datos$duration, times = 1, p = 0.8, list = FALSE)
datostra = datos[trainIndex,]
datostst = datos[-trainIndex,]
# Paso 5: Llame a la funciÃ³n naiveBayes, de la librerÃ­a "e1071" para entrenar el modelo con los
# datos de entrenamiento (datostra)
# --> modelo1=naiveBayes(........)
modelo1 = naiveBayes(imdb_score ~ ., datostra, laplace = 0)
# Paso 6: Llame a la funciÃ³n naiveBayes para entrenar un modelo2, utilizando el parÃ¡metro que aplica
# la correcciÃ³n de Laplace
# --> modelo2=naiveBayes(...)
modelo2 = naiveBayes(imdb_score ~ ., datostra, laplace = 3)
# Paso 7: Realice predicciones sobre los datos de test (datostst) de los dos modelos con el comando predict
# --> prediccion1=predict(modelo1,...)
# --> prediccion2=predict(modelo2,...)
prediccion1 = predict(modelo1, datostst)
prediccion2 = predict(modelo2, datostst)
table(prediccion1)
table(prediccion2)
#---------------------------------------------------------------------------
######################## Parte 2 ###########################################
#---------------------------------------------------------------------------
# Paso 1: Utilice la funciÃ³n read.csv para leer el contenido del fichero "movie_metadata.csv" (igual que antes)
# En este caso, elimine toda columna que no sea numÃ©rica y toda fila que tenga algÃºn valor perdido.
# Discretice la columna imbd_score igual que antes
# * Puede utilizar los comandos is.numeric()
# * EstÃ¡ permitido eliminar "manualmente" las columnas, pero no es elegante
datos = read.csv("datos/movie_metadata.csv")
datos = na.omit(datos)
datos = datos[,sapply(datos, is.numeric)]
datos$imdb_score = discretize(datos$imdb_score, "fixed", breaks = c(0,7.5,9,10))
# Paso 2: Cree una copia de los datos llamada "datosnor" y normalizela
# * Puede usar la funciÃ³n "data.Normalization()", dentro del paquete "clusterSim"
# * ojo, lea la documentaciÃ³n para ver quÃ© valor tiene que darle al parÃ¡metro "type"
# * Cuidado, te darÃ¡ error si intentas normalizarlo todo, ya que imbd_score no es numÃ©rica
index = match("imdb_score", names(datos))
datosnor = datos[,-index]
#!!!!!!!! REVISAR NO SE QUE TIPO DE NORMALIZACION NI SI ES COLUMNA O ROW
datosnor = data.Normalization(datosnor,"n1","column")
datosnor$imdb_score = datos$imdb_score;
# Paso 3: (igual que antes) utilice el mÃ©todo "createDataPartition", de la librerÃ­a "caret" para partir los datos
#   en 2 pedazos-> [datostra, con el 80% de los datos] [datostst con el 20% restante]
# * Ojo, para ejecutar KNN la clase tiene que estar por un lado, y la columna a predecir separada, por otro
# * por ello, deberÃ¡ partir datostra en [datostra, con todas las columnas, salvo imbd_score] [labeltra, con Ãºnicamente la columna imbd_score]
# * (idem para datostst), y tenga en cuenta que debe realizar el mismo proceso con datosnor
trainIndex = createDataPartition(datos$duration, times = 1, p = 0.8, list = FALSE)
datostra = datos[trainIndex,]
labeltra = matrix(datostra$imdb_score)
index = match("imdb_score", names(datostra))
datostra = datostra[,-index]
datostst = datos[-trainIndex,]
labeltst = matrix(datostst$imdb_score)
index = match("imdb_score", names(datostst))
datostst = datostst[,-index]
trainIndex = createDataPartition(datosnor$duration, times = 1, p = 0.8, list = FALSE)
datostranor = datosnor[trainIndex,]
labeltranor = matrix(datostranor$imdb_score)
index = match("imdb_score", names(datostranor))
datostranor = datostranor[,-index]
datoststnor = datosnor[-trainIndex,]
labeltstnor = matrix(datoststnor$imdb_score)
index = match("imdb_score", names(datoststnor))
datoststnor = datoststnor[,-index]
# Paso 4: Aplique KNN con 3 valores de K diferentes (a su elecciÃ³n), para los datos normalizados y sin normalizar
# --> prediccion1 = knn(datostra, datostst, labeltra, k = XX)
# --> prediccion2 = knn(datostranor, datoststnor, labeltranor, k = XX)
# (hasta 6)
prediccion1 = knn(datostra, datostst, labeltra, k=10)
prediccion2 = knn(datostra, datostst, labeltra, k=1)
prediccion3 = knn(datostra, datostst, labeltra, k=5)
table(prediccion1)
table(prediccion2)
table(prediccion3)
prediccion4 = knn(datostranor, datoststnor, labeltranor, k=10)
prediccion5 = knn(datostranor, datoststnor, labeltranor, k=1)
prediccion6 = knn(datostranor, datoststnor, labeltranor, k=5)
table(prediccion4)
table(prediccion5)
table(prediccion6)
View(datos)
